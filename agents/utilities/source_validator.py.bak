"""
Source validation and blacklist system for the Blog Accelerator Agent.

This utility:
1. Validates sources against credibility criteria
2. Maintains a blacklist of unreliable domains
3. Provides credibility scoring for research sources
"""

import os
import re
import json
import logging
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import requests
from urllib.parse import urlparse
from collections import deque
import asyncio

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SourceValidationError(Exception):
    """Exception raised for errors in source validation."""
    pass


class RateLimiter:
    """
    Simple rate limiter for API calls.
    
    Limits calls to a specified rate per second.
    """
    
    def __init__(self, max_calls_per_second: int = 1, buffer_time: float = 0.1):
        """
        Initialize the rate limiter.
        
        Args:
            max_calls_per_second: Maximum calls allowed per second
            buffer_time: Additional buffer time in seconds for more reliability
        """
        self.max_calls_per_second = max_calls_per_second
        self.buffer_time = buffer_time
        self.call_timestamps = deque()
        # Adaptive rate limiting variables
        self.rate_limit_errors = 0
        self.last_rate_limit_error = 0
        self.adaptive_mode = False
        self.original_rate = max_calls_per_second
    
    async def wait_if_needed(self):
        """
        Wait if necessary to stay within rate limits.
        """
        now = time.time()
        
        # Remove timestamps older than 1 second
        while self.call_timestamps and now - self.call_timestamps[0] > 1:
            self.call_timestamps.popleft()
        
        # Check if we need to adapt the rate limit
        if self.rate_limit_errors >= 3 and now - self.last_rate_limit_error < 10:
            if not self.adaptive_mode:
                # Enter adaptive mode - reduce rate
                new_rate = max(1, self.max_calls_per_second // 2)
                logger.warning(f"Too many rate limit errors. Adapting rate limit from {self.max_calls_per_second} to {new_rate} req/s")
                self.adaptive_mode = True
                self.max_calls_per_second = new_rate
        
        # If no recent rate limit errors, gradually recover
        elif self.adaptive_mode and now - self.last_rate_limit_error > 60:
            # Try to recover by increasing the rate limit
            new_rate = min(self.original_rate, self.max_calls_per_second * 2)
            if new_rate > self.max_calls_per_second:
                logger.info(f"Recovering: increasing rate limit from {self.max_calls_per_second} to {new_rate} req/s")
                self.max_calls_per_second = new_rate
                # Reset if we've recovered fully
                if self.max_calls_per_second >= self.original_rate:
                    self.adaptive_mode = False
                    self.rate_limit_errors = 0
                    logger.info(f"Returned to original rate limit: {self.original_rate} req/s")
        
        # Check if we need to wait (over the limit)
        current_rate = len(self.call_timestamps)
        if current_rate >= self.max_calls_per_second:
            # Calculate wait time
            earliest_timestamp = min(self.call_timestamps)
            wait_time = 1.0 - (now - earliest_timestamp) + self.buffer_time
            if wait_time > 0:
                logger.debug(f"Rate limit reached, waiting {wait_time:.2f} seconds")
                await asyncio.sleep(wait_time)
        else:
            # Even if under the limit, add a small delay for more reliable spacing
            await asyncio.sleep(self.buffer_time)
        
        # Add current timestamp (after waiting)
        self.call_timestamps.append(time.time())
    
    def record_rate_limit_error(self):
        """Record a rate limit error for adaptive rate limiting."""
        self.rate_limit_errors += 1
        self.last_rate_limit_error = time.time()
        logger.warning(f"Rate limit error recorded ({self.rate_limit_errors} in total)")


class SourceValidator:
    """
    Source validator for checking credibility of research sources.
    
    Features:
    - Domain blacklisting
    - Credibility scoring
    - Academic source validation
    - Recency checks
    """
    
    def __init__(
        self,
        blacklist_path: Optional[str] = None,
        cache_dir: Optional[str] = None,
        brave_api_key: Optional[str] = None,
        max_brave_calls_per_second: int = 20
    ):
        """
        Initialize the source validator.
        
        Args:
            blacklist_path: Path to JSON file containing blacklisted domains
            cache_dir: Directory to cache validation results
            brave_api_key: API key for Brave Search (for enhanced validation)
            max_brave_calls_per_second: Maximum Brave API calls per second (default: 20 for Premium plan)
        """
        self.brave_api_key = brave_api_key or os.environ.get("BRAVE_API_KEY")
        
        # Set default paths if not provided
        self.blacklist_path = blacklist_path or os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "../../data/blacklist.json"
        )
        self.cache_dir = cache_dir or os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "../../data/validation_cache"
        )
        
        # Ensure cache directory exists
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # Initialize rate limiter
        self.rate_limiter = RateLimiter(max_calls_per_second=max_brave_calls_per_second)
        
        # Initialize search cache
        self.search_cache = {}
        
        # Load blacklist
        self.blacklist = self._load_blacklist()
        
        # Define credibility tiers
        self.credibility_tiers = {
            "academic": ["edu", "gov", "org"],
            "high": ["nature.com", "science.org", "nejm.org", "thelancet.com"],
            "news": ["nytimes.com", "washingtonpost.com", "bbc.co.uk", "reuters.com"],
            "medium": ["medium.com", "substack.com"],
            "low": ["wordpress.com", "blogspot.com"],
            "untrusted": []  # Populated from blacklist
        }
        
        # Add blacklisted domains to untrusted tier
        self.credibility_tiers["untrusted"].extend(self.blacklist)
    
    def _load_blacklist(self) -> List[str]:
        """
        Load blacklisted domains from JSON file.
        
        Returns:
            List of blacklisted domains
        """
        try:
            if os.path.exists(self.blacklist_path):
                with open(self.blacklist_path, 'r') as f:
                    return json.load(f)
            else:
                # Create default blacklist if not exists
                default_blacklist = [
                    "example.com",
                    "untrusted-source.com",
                    "fake-news.org",
                    "conspiracy-theories.net"
                ]
                with open(self.blacklist_path, 'w') as f:
                    json.dump(default_blacklist, f, indent=2)
                return default_blacklist
        except Exception as e:
            logger.error(f"Error loading blacklist: {e}")
            return []
    
    def save_blacklist(self) -> None:
        """Save the current blacklist to disk."""
        try:
            with open(self.blacklist_path, 'w') as f:
                json.dump(self.blacklist, f, indent=2)
            logger.info(f"Blacklist saved to {self.blacklist_path}")
        except Exception as e:
            logger.error(f"Error saving blacklist: {e}")
            raise SourceValidationError(f"Failed to save blacklist: {e}")
    
    def add_to_blacklist(self, domain: str) -> None:
        """
        Add a domain to the blacklist.
        
        Args:
            domain: Domain to blacklist
        """
        # Extract domain if full URL is provided
        parsed = urlparse(domain)
        domain_only = parsed.netloc or parsed.path
        
        # Remove www. prefix if present
        domain_only = re.sub(r'^www\.', '', domain_only)
        
        if domain_only not in self.blacklist:
            self.blacklist.append(domain_only)
            self.credibility_tiers["untrusted"].append(domain_only)
            self.save_blacklist()
            logger.info(f"Added {domain_only} to blacklist")
    
    def remove_from_blacklist(self, domain: str) -> bool:
        """
        Remove a domain from the blacklist.
        
        Args:
            domain: Domain to remove
            
        Returns:
            True if domain was removed, False if not in blacklist
        """
        # Handle www. prefix and full URLs
        parsed = urlparse(domain)
        domain_only = parsed.netloc or parsed.path
        domain_only = re.sub(r'^www\.', '', domain_only)
        
        if domain_only in self.blacklist:
            self.blacklist.remove(domain_only)
            if domain_only in self.credibility_tiers["untrusted"]:
                self.credibility_tiers["untrusted"].remove(domain_only)
            self.save_blacklist()
            logger.info(f"Removed {domain_only} from blacklist")
            return True
        return False
    
    def is_blacklisted(self, url: str) -> bool:
        """
        Check if a URL's domain is blacklisted.
        
        Args:
            url: URL to check
            
        Returns:
            True if domain is blacklisted, False otherwise
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc or parsed.path
            domain = re.sub(r'^www\.', '', domain)
            
            # Check if domain or any parent domain is blacklisted
            domain_parts = domain.split('.')
            for i in range(len(domain_parts) - 1):
                check_domain = '.'.join(domain_parts[i:])
                if check_domain in self.blacklist:
                    return True
            
            return False
        except Exception as e:
            logger.error(f"Error checking blacklist for {url}: {e}")
            return False
    
    def get_domain_tier(self, url: str) -> str:
        """
        Get the credibility tier for a domain.
        
        Args:
            url: URL to check
            
        Returns:
            Credibility tier ('academic', 'high', 'news', 'medium', 'low', 'untrusted')
        """
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path
        domain = re.sub(r'^www\.', '', domain)
        
        # Check if blacklisted
        if self.is_blacklisted(url):
            return "untrusted"
        
        # Check for academic/government domains
        domain_parts = domain.split('.')
        tld = domain_parts[-1] if len(domain_parts) > 1 else ""
        
        if any(domain.endswith(f".{suffix}") for suffix in self.credibility_tiers["academic"]):
            return "academic"
        
        # Check other tiers
        for tier, domains in self.credibility_tiers.items():
            if tier == "untrusted" or tier == "academic":
                continue
            
            if any(domain.endswith(trusted_domain) for trusted_domain in domains):
                return tier
        
        # Default to low if not found in other tiers
        return "low"
    
    def get_credibility_score(self, url: str) -> int:
        """
        Get a numeric credibility score for a URL.
        
        Args:
            url: URL to score
            
        Returns:
            Score from 0-10 where 10 is highest credibility
        """
        tier = self.get_domain_tier(url)
        
        # Map tiers to scores
        tier_scores = {
            "academic": 10,
            "high": 9,
            "news": 8,
            "medium": 6,
            "low": 4,
            "untrusted": 0
        }
        
        return tier_scores.get(tier, 5)
    
    def validate_source(self, source: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate a source and enrich with credibility information.
        
        Args:
            source: Source dict with url, title, etc.
            
        Returns:
            Enriched source with validation data
            
        Raises:
            SourceValidationError: If source is invalid or fails validation
        """
        url = source.get('url')
        if not url:
            raise SourceValidationError("Source missing URL")
        
        # Check if blacklisted
        if self.is_blacklisted(url):
            raise SourceValidationError(f"Source domain is blacklisted: {url}")
        
        # Get basic domain info
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path
        domain = re.sub(r'^www\.', '', domain)
        
        # Get credibility info
        tier = self.get_domain_tier(url)
        score = self.get_credibility_score(url)
        
        # Try to determine publication date
        pub_date = source.get('date')
        
        # Enrich source with validation data
        enriched = {
            **source,
            'validation': {
                'domain': domain,
                'credibility_tier': tier,
                'credibility_score': score,
                'blacklisted': False,
                'publication_date': pub_date,
                'validated_at': datetime.now().isoformat()
            }
        }
        
        return enriched
    
    async def find_supporting_contradicting_sources(
        self, 
        claim: str, 
        count: int = 3
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Find sources that support or contradict a claim.
        
        Args:
            claim: Claim to search for
            count: Number of sources to find of each type
            
        Returns:
            Tuple of (supporting_sources, contradicting_sources)
        """
        # Use more targeted search queries
        supporting_query = f"evidence supporting {claim}"
        contradicting_query = f"evidence against {claim}"
        
        supporting = []
        contradicting = []
        
        if not self.brave_api_key:
            logger.warning("Brave API key not available. Using mock data.")
            # Return mock data
            mock_supporting = [
                {
                    'title': f'Supporting source for "{claim[:30]}..." #{i}',
                    'url': f'https://example.com/supporting{i}',
                    'description': f'This is a mock supporting source #{i} for claim: {claim[:50]}...',
                    'source': 'Mock Supporting Source',
                    'date': datetime.now().isoformat()
                } for i in range(1, count + 1)
            ]
            
            mock_contradicting = [
                {
                    'title': f'Contradicting source for "{claim[:30]}..." #{i}',
                    'url': f'https://example.com/contradicting{i}',
                    'description': f'This is a mock contradicting source #{i} for claim: {claim[:50]}...',
                    'source': 'Mock Contradicting Source',
                    'date': datetime.now().isoformat()
                } for i in range(1, count + 1)
            ]
            
            return mock_supporting, mock_contradicting
        
        max_retries = 3

        try:
            # Fetch supporting sources
            headers = {
                'Accept': 'application/json',
                'X-Subscription-Token': self.brave_api_key
            }
            
            for query, results_list in [
                (supporting_query, supporting),
                (contradicting_query, contradicting)
            ]:
                # Check if we need to make this call
                # Skip if we already have enough high-quality results
                if len(results_list) >= count:
                    continue
                
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        params = {
                            'q': query,
                            'count': count * 2,  # Request more to filter blacklisted
                            'search_lang': 'en'
                        }
                        
                        # Apply rate limiting
                        await self.rate_limiter.wait_if_needed()
                        
                        response = requests.get(
                            'https://api.search.brave.com/res/v1/web/search',
                            headers=headers,
                            params=params
                        )
                        
                        if response.status_code == 429:
                            # Rate limit hit
                            retry_count += 1
                            if retry_count < max_retries:
                                wait_time = (2 ** retry_count) * 2  # Exponential backoff
                                logger.warning(f"Brave API rate limit hit, retrying in {wait_time} seconds (attempt {retry_count}/{max_retries})")
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                logger.error(f"Brave API rate limit exceeded after {max_retries} retries")
                                # Create fallback results since we can't get real ones
                                if 'supporting' in query:
                                    # Add fallback supporting sources
                                    for i in range(1, count + 1):
                                        results_list.append({
                                            'title': f'Rate-limited fallback supporting source #{i}',
                                            'url': f'https://example.com/supporting{i}',
                                            'description': f'This is a fallback supporting source due to rate limit: {claim[:50]}...',
                                            'source': 'Rate-Limited Fallback',
                                            'date': datetime.now().isoformat()
                                        })
                                else:
                                    # Add fallback contradicting sources
                                    for i in range(1, count + 1):
                                        results_list.append({
                                            'title': f'Rate-limited fallback contradicting source #{i}',
                                            'url': f'https://example.com/contradicting{i}',
                                            'description': f'This is a fallback contradicting source due to rate limit: {claim[:50]}...',
                                            'source': 'Rate-Limited Fallback',
                                            'date': datetime.now().isoformat()
                                        })
                                # Break out of retry loop
                                break
                        
                        elif response.status_code != 200:
                            logger.error(f"Error searching Brave API: {response.status_code} - {response.text}")
                            raise SourceValidationError(f"Brave Search API error: {response.status_code}")
                        
                        results = response.json().get('web', {}).get('results', [])
                        
                        # Process results
                        for result in results:
                            url = result.get('url')
                            
                            # Skip blacklisted sources
                            if self.is_blacklisted(url):
                                continue
                            
                            source = {
                                'title': result.get('title'),
                                'url': url,
                                'description': result.get('description'),
                                'source': result.get('extra_snippets', {}).get('source', 'Unknown Source'),
                                'date': datetime.now().isoformat()
                            }
                            
                            # Add validation data
                            source = self.validate_source(source)
                            results_list.append(source)
                            
                            # Stop when we have enough
                            if len(results_list) >= count:
                                break
                        
                        # Successfully got results, break out of retry loop
                        break
                    
                    except Exception as e:
                        retry_count += 1
                        if retry_count < max_retries:
                            wait_time = (2 ** retry_count) * 2  # Exponential backoff
                            logger.warning(f"Error in searching for sources, retrying in {wait_time} seconds (attempt {retry_count}/{max_retries})")
                            await asyncio.sleep(wait_time)
                        else:
                            logger.error(f"Failed to find sources after {max_retries} retries: {e}")
                            raise SourceValidationError(f"Failed to find sources: {e}")
            
            # Return consolidated results
            return supporting, contradicting
            
        except Exception as e:
            logger.error(f"Error finding supporting/contradicting sources: {e}")
            raise SourceValidationError(f"Failed to find sources: {e}")

    async def search_web(self, query: str, count: int = 5) -> List[Dict[str, Any]]:
        """
        Search the web for a query using Brave Search API.
        Optimized for Premium Plan (20 requests per second).
        
        Args:
            query: Search query
            count: Number of results to return
            
        Returns:
            List of search result dictionaries
        """
        # Check cache before making API calls
        cache_key = f"web:{query}:count:{count}"
        if cache_key in self.search_cache:
            logger.info(f"Using cached results for web search: {query}")
            return self.search_cache[cache_key]
            
        if not self.brave_api_key:
            logger.warning("Brave API key not available. Using mock data.")
            # Return mock data
            mock_results = [
                {
                    'title': f'Result for "{query}" #{i}',
                    'url': f'https://example.com/result{i}',
                    'description': f'This is a mock search result #{i} for query: {query}',
                    'source': 'Mock Source',
                    'date': datetime.now().isoformat()
                } for i in range(1, count + 1)
            ]
            # Cache the mock results
            self.search_cache[cache_key] = mock_results
            return mock_results
        
        max_retries = 5  # Keep 5 retries for robustness
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # Prepare request
                headers = {
                    'Accept': 'application/json',
                    'X-Subscription-Token': self.brave_api_key
                }
                
                params = {
                    'q': query,
                    'count': count * 2,  # Request more to filter blacklisted
                    'search_lang': 'en'
                }
                
                # Apply rate limiting - now use the async version
                await self.rate_limiter.wait_if_needed()
                
                # Add a small delay based on retry count (much shorter for Premium plan)
                if retry_count > 0:
                    await asyncio.sleep(retry_count * 0.2)  # Shorter delays - 0.2s, 0.4s, 0.6s, 0.8s
                
                # Make request
                response = requests.get(
                    'https://api.search.brave.com/res/v1/web/search',
                    headers=headers,
                    params=params,
                    timeout=10  # Add timeout to prevent hanging requests
                )
                
                if response.status_code == 429:
                    # Rate limit hit - unusual for Premium plan but still possible
                    self.rate_limiter.record_rate_limit_error()  # Record the error for adaptive rate limiting
                    retry_count += 1
                    if retry_count < max_retries:
                        # Less steep backoff for Premium plan
                        wait_time = 0.5 + (retry_count * 0.5)  # Start with 1s, then 1.5s, 2s, 2.5s
                        logger.warning(f"Brave API rate limit hit, retrying in {wait_time} seconds (attempt {retry_count}/{max_retries}) - this is unusual for Premium plan")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"Brave API rate limit exceeded after {max_retries} retries - check if your Premium plan key is valid")
                        # Fall back to mock results when rate limit persists
                        mock_results = [
                            {
                                'title': f'Rate-limited fallback for "{query}" #{i}',
                                'url': f'https://example.com/ratelimited{i}',
                                'description': f'This is a fallback result #{i} due to API rate limits for query: {query}',
                                'source': 'Rate-Limited Fallback',
                                'date': datetime.now().isoformat()
                            } for i in range(1, count + 1)
                        ]
                        # Cache the fallback results
                        self.search_cache[cache_key] = mock_results
                        return mock_results
                
                elif response.status_code != 200:
                    logger.error(f"Error searching Brave API: {response.status_code} - {response.text}")
                    retry_count += 1
                    if retry_count < max_retries and response.status_code >= 500:  # Only retry server errors
                        wait_time = 0.5 + (retry_count * 0.5)  # Less steep backoff
                        logger.warning(f"Server error, retrying in {wait_time} seconds (attempt {retry_count}/{max_retries})")
                        await asyncio.sleep(wait_time)
                        continue
                    raise SourceValidationError(f"Brave Search API error: {response.status_code}")
                
                # Process results
                results = []
                try:
                    web_results = response.json().get('web', {}).get('results', [])
                except ValueError as e:
                    logger.error(f"Error parsing JSON response: {e}")
                    raise SourceValidationError(f"Failed to parse API response: {e}")
                
                for result in web_results:
                    url = result.get('url')
                    
                    # Skip blacklisted sources
                    if self.is_blacklisted(url):
                        continue
                    
                    source = {
                        'title': result.get('title'),
                        'url': url,
                        'description': result.get('description'),
                        'source': result.get('extra_snippets', {}).get('source', 'Unknown Source'),
                        'date': datetime.now().isoformat()
                    }
                    
                    # Add validation data
                    source = self.validate_source(source)
                    results.append(source)
                    
                    # Stop when we have enough
                    if len(results) >= count:
                        break
                
                # Cache the results
                self.search_cache[cache_key] = results
                logger.info(f"Successfully found {len(results)} search results for query: {query}")
                return results
                
            except Exception as e:
                logger.error(f"Error searching web: {e}")
                retry_count += 1
                if retry_count < max_retries:
                    wait_time = 0.5 + (retry_count * 0.5)  # Less steep backoff
                    logger.warning(f"Error in web search, retrying in {wait_time} seconds (attempt {retry_count}/{max_retries})")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"Web search failed after {max_retries} retries: {e}")
                    # Return fallback results instead of raising an exception
                    mock_results = [
                        {
                            'title': f'Error fallback for "{query}" #{i}',
                            'url': f'https://example.com/error{i}',
                            'description': f'This is a fallback result #{i} due to errors for query: {query}. Error: {str(e)}',
                            'source': 'Error Fallback',
                            'date': datetime.now().isoformat()
                        } for i in range(1, count + 1)
                    ]
                    self.search_cache[cache_key] = mock_results
                    return mock_results
    
    def calculate_consensus_score(
        self,
        supporting_sources: List[Dict[str, Any]],
        contradicting_sources: List[Dict[str, Any]]
    ) -> int:
        """
        Calculate a consensus score based on supporting and contradicting sources.
        
        Args:
            supporting_sources: List of supporting sources
            contradicting_sources: List of contradicting sources
            
        Returns:
            Consensus score from 1-10:
            - 10: Strong consensus supporting the claim
            - 1: Strong consensus against the claim
            - 5: Evenly divided evidence
        """
        if not supporting_sources and not contradicting_sources:
            return 5  # Neutral when no sources
        
        # Calculate weighted scores based on credibility
        supporting_score = sum(
            source.get('validation', {}).get('credibility_score', 5)
            for source in supporting_sources
        )
        
        contradicting_score = sum(
            source.get('validation', {}).get('credibility_score', 5)
            for source in contradicting_sources
        )
        
        # Calculate total possible score
        total_sources = len(supporting_sources) + len(contradicting_sources)
        max_possible = total_sources * 10  # If all sources had max credibility
        
        if max_possible == 0:
            return 5  # Avoid division by zero
        
        # Calculate percentage supporting
        supporting_percentage = supporting_score / (supporting_score + contradicting_score)
        
        # Map percentage to 1-10 scale
        # 0% supporting -> 1
        # 50% supporting -> 5
        # 100% supporting -> 10
        consensus_score = round(1 + supporting_percentage * 9)
        
        return consensus_score 